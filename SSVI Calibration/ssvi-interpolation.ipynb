{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14141187,"sourceType":"datasetVersion","datasetId":9011859},{"sourceId":14141203,"sourceType":"datasetVersion","datasetId":9011871}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the pandas library (aliased as 'pd') for working with tabular data structures\nimport pandas as pd\n\n# Import the NumPy library (aliased as 'np') for numerical computing:\nimport numpy as np\n\n# Import SciPy's 'griddata' function, which interpolates scattered (x, y, ...) data\nfrom scipy.interpolate import griddata\n\n# Import SciPy's 'least_squares' optimizer, which solves non-linear least-squares problems:\nfrom scipy.optimize import least_squares\n\n# Import SciPy's 'minimize_scalar' optimizer, which performs 1D optimization:\nfrom scipy.optimize import minimize_scalar","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:10.437854Z","iopub.execute_input":"2025-12-28T22:26:10.438200Z","iopub.status.idle":"2025-12-28T22:26:11.113462Z","shell.execute_reply.started":"2025-12-28T22:26:10.438173Z","shell.execute_reply":"2025-12-28T22:26:11.111907Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Read the training option-chain dataset from a CSV file into a pandas DataFrame.\noption_chains_train = pd.read_csv(\"/kaggle/input/raw-option-chains/aapl_2016_2020.csv\")\n\n# Read the test option-chain dataset from a CSV file into a pandas DataFrame.\noption_chains_test = pd.read_csv(\"/kaggle/input/raw-option-chains/aapl_2021_2023.csv\")\n\n# Read the Fed Funds rate history from a CSV file into a pandas DataFrame.\nrisk_free_rates = pd.read_csv(\"/kaggle/input/fed-funds-rates/FedFunds History.csv\", sep=\";\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:11.115902Z","iopub.execute_input":"2025-12-28T22:26:11.116403Z","iopub.status.idle":"2025-12-28T22:26:26.212021Z","shell.execute_reply.started":"2025-12-28T22:26:11.116376Z","shell.execute_reply":"2025-12-28T22:26:26.210892Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/947672888.py:2: DtypeWarning: Columns (8,9,10,11,12,15,17,18,20,21,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n  option_chains_train = pd.read_csv(\"/kaggle/input/raw-option-chains/aapl_2016_2020.csv\")\n/tmp/ipykernel_47/947672888.py:5: DtypeWarning: Columns (15,17,18,20,21,23) have mixed types. Specify dtype option on import or set low_memory=False.\n  option_chains_test = pd.read_csv(\"/kaggle/input/raw-option-chains/aapl_2021_2023.csv\")\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Define a function that cleans and standardizes a raw option-chain DataFrame\ndef clean_data(df):\n    # Specify the exact subset of raw columns to keep from the original dataset\n    cols = [\" [QUOTE_DATE]\", \" [EXPIRE_DATE]\", \" [DTE]\", \" [C_IV]\", \" [C_BID]\", \" [C_ASK]\",\n            \" [STRIKE]\", \" [P_BID]\", \" [P_ASK]\", \" [P_IV]\", \" [UNDERLYING_LAST]\"]\n    # Filter the DataFrame to retain only the selected columns\n    df = df[cols]\n    # Rename the columns to cleaner, standardized names without brackets or leading spaces\n    df.columns = [\"QUOTE_DATE\", \"EXPIRE_DATE\", \"DTE\", \"C_IV\", \"C_BID\", \"C_ASK\",\n                  \"STRIKE\", \"P_BID\", \"P_ASK\", \"P_IV\", \"UNDERLYING_LAST\"]\n    # Drop rows containing any missing values to ensure numerical consistency\n    df = df.dropna()\n    # Convert the quote date column from string/object to pandas datetime format\n    df[\"QUOTE_DATE\"] = pd.to_datetime(df[\"QUOTE_DATE\"])\n    # Convert the option expiration date column to pandas datetime format\n    df[\"EXPIRE_DATE\"] = pd.to_datetime(df[\"EXPIRE_DATE\"])\n    # Compute time-to-maturity in years by rounding DTE (days to expiration) and dividing by 365\n    df[\"TTM\"] = df[\"DTE\"].round() / 365\n    # Compute relative strike (moneyness) as strike price divided by the underlying spot price\n    df[\"RELATIVE_STRIKE\"] = df[\"STRIKE\"] / df[\"UNDERLYING_LAST\"]\n    # Convert call implied volatility to numeric type, coercing invalid entries to NaN\n    df[\"C_IV\"] = df[\"C_IV\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Convert call bid prices to numeric type, coercing invalid entries to NaN\n    df[\"C_BID\"] = df[\"C_BID\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Convert call ask prices to numeric type, coercing invalid entries to NaN\n    df[\"C_ASK\"] = df[\"C_ASK\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Convert put implied volatility to numeric type, coercing invalid entries to NaN\n    df[\"P_IV\"] = df[\"P_IV\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Convert put bid prices to numeric type, coercing invalid entries to NaN\n    df[\"P_BID\"] = df[\"P_BID\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Convert put ask prices to numeric type, coercing invalid entries to NaN\n    df[\"P_ASK\"] = df[\"P_ASK\"].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n    # Drop any rows that became invalid after numeric type coercion\n    df = df.dropna()\n    # Return the cleaned and standardized DataFrame\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.213279Z","iopub.execute_input":"2025-12-28T22:26:26.213701Z","iopub.status.idle":"2025-12-28T22:26:26.224920Z","shell.execute_reply.started":"2025-12-28T22:26:26.213675Z","shell.execute_reply":"2025-12-28T22:26:26.223817Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define a function to clean and convert the raw risk-free rate time series into a daily, fully populated dataset\ndef rates_clean(df):\n  # Convert the \"Date\" column from string/object into pandas datetime for proper time-series handling\n  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n  # Convert the \"Rate\" column from comma-decimal strings to floats and scale from percent to decimal (e.g., 5.25 -> 0.0525)\n  df[\"Rate\"] = df[\"Rate\"].str.replace(\",\", \".\").astype(float) / 100\n  # Sort by date and set \"Date\" as the index to enable time-based reindexing and interpolation\n  df = df.sort_values(\"Date\").set_index(\"Date\")\n  # Create a complete daily date index spanning from the first to the last observed date\n  full_idx = pd.date_range(df.index.min(), df.index.max(), freq=\"D\")\n  # Reindex to the full daily calendar, introducing NaNs for dates that were missing in the original series\n  df_mod = df.reindex(full_idx)\n  # Identify numeric columns (here, primarily \"Rate\") so interpolation is applied only to numeric data\n  num_cols = df_mod.select_dtypes(\"number\").columns\n  # Fill missing numeric values using time-aware interpolation, then forward-fill and back-fill edge gaps\n  df_mod[num_cols] = df_mod[num_cols].interpolate(method=\"time\").ffill().bfill()\n  # Convert the index back into a regular \"Date\" column and restore a standard DataFrame structure\n  df_mod = df_mod.rename_axis(\"Date\").reset_index()\n  # Return the cleaned, daily, fully populated rate DataFrame\n  return df_mod","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.226925Z","iopub.execute_input":"2025-12-28T22:26:26.227213Z","iopub.status.idle":"2025-12-28T22:26:26.255858Z","shell.execute_reply.started":"2025-12-28T22:26:26.227190Z","shell.execute_reply":"2025-12-28T22:26:26.254622Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define a function that augments an option-chain DataFrame (already merged with rates) with forward/moneyness/variance features\ndef df_option_chains_rates_cleaning(df):\n    # Compute the forward price F = S * exp(r*T) using underlying spot, continuously compounded rate, and time-to-maturity\n    df[\"F\"] = df[\"UNDERLYING_LAST\"] * np.exp((df[\"Rate\"]) * df[\"TTM\"])\n    # Compute log-forward moneyness k = ln(K/F), where K is strike and F is the forward price\n    df[\"LogMK\"] = np.log(df[\"STRIKE\"] / df[\"F\"])\n    # Compute total implied variance w = (IV^2) * T using call implied vol and time-to-maturity\n    df[\"w\"] = (df[\"C_IV\"] ** 2) * df[\"TTM\"]\n    # Drop the redundant \"Date\" column (typically kept only for the merge key with the rate series)\n    df.drop(columns=[\"Date\"], inplace=True)\n    # Return the enriched DataFrame with the newly created columns\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.256877Z","iopub.execute_input":"2025-12-28T22:26:26.257129Z","iopub.status.idle":"2025-12-28T22:26:26.283021Z","shell.execute_reply.started":"2025-12-28T22:26:26.257109Z","shell.execute_reply":"2025-12-28T22:26:26.281667Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define a function to extract all unique option quote dates in chronological order\ndef dates_extraction(df):\n  # Select the QUOTE_DATE column, remove duplicates, and sort the dates in ascending order\n  dates = df[\"QUOTE_DATE\"].drop_duplicates().sort_values(ascending=True)\n  # Return the sorted series of unique quote dates\n  return dates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.284376Z","iopub.execute_input":"2025-12-28T22:26:26.284716Z","iopub.status.idle":"2025-12-28T22:26:26.309666Z","shell.execute_reply.started":"2025-12-28T22:26:26.284686Z","shell.execute_reply":"2025-12-28T22:26:26.308557Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define a function to retrieve the underlying spot price for a given quote date\ndef get_price(df, date):\n  # Filter rows matching the specified quote date, select the underlying price,\n  # and extract the first value (prices are constant within a given quote date)\n  price = df.loc[df[\"QUOTE_DATE\"].eq(date), \"UNDERLYING_LAST\"].iloc[0]\n  # Return the spot price for the specified date\n  return price","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.311116Z","iopub.execute_input":"2025-12-28T22:26:26.311510Z","iopub.status.idle":"2025-12-28T22:26:26.330150Z","shell.execute_reply.started":"2025-12-28T22:26:26.311477Z","shell.execute_reply":"2025-12-28T22:26:26.328775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define a function to retrieve the risk-free interest rate for a given date\ndef get_rate(df, date):\n  # Filter rows matching the specified date, select the corresponding rate,\n  # and extract the first value (rates are unique per calendar date)\n  rate = df.loc[df[\"Date\"].eq(date), \"Rate\"].iloc[0]\n  # Return the risk-free rate for the specified date\n  return rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.331295Z","iopub.execute_input":"2025-12-28T22:26:26.331623Z","iopub.status.idle":"2025-12-28T22:26:26.353575Z","shell.execute_reply.started":"2025-12-28T22:26:26.331600Z","shell.execute_reply":"2025-12-28T22:26:26.352456Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Compute eSSVI total variance w(k) for given log-forward moneyness k and parameters (theta, psi, rho)\ndef essvi_total_variance(k, theta, psi, rho):\n    # Ensure k is a NumPy float array so vectorized operations behave consistently\n    k = np.asarray(k, dtype=float)\n    # Enforce basic parameter admissibility; if violated, return NaNs with the same shape as k\n    if theta <= 0.0 or psi <= 0.0 or abs(rho) >= 1.0:\n        return np.full_like(k, np.nan, dtype=float)\n    # Compute phi = psi/theta (a common eSSVI re-parameterization)\n    phi = psi / theta\n    # Compute the linear term x = phi*k + rho used inside the square-root expression\n    x = phi * k + rho\n    # Compute the square-root argument: x^2 + (1 - rho^2) which is always nonnegative in theory\n    inside = x * x + (1.0 - rho * rho)\n    # Numerically floor the argument to avoid sqrt of tiny negative values due to floating point error\n    inside = np.maximum(inside, 1e-16)\n    # Apply the eSSVI formula for total variance w(k)\n    w = 0.5 * theta * (1.0 + rho * phi * k + np.sqrt(inside))\n    # Return the total variance array w(k)\n    return w\n\n# Interpolate eSSVI parameters to a target maturity T_target using calibrated arrays at maturities Ts\ndef interpolate_essvi_params(T_target, Ts, theta, psi, rho):\n    # Convert maturities array to float NumPy array for safe numeric comparisons and indexing\n    Ts = np.asarray(Ts, dtype=float)\n    # Convert theta array to float NumPy array\n    theta = np.asarray(theta, dtype=float)\n    # Convert psi array to float NumPy array\n    psi = np.asarray(psi, dtype=float)\n    # Convert rho array to float NumPy array\n    rho = np.asarray(rho, dtype=float)\n    # Ensure T_target is a scalar float\n    T_target = float(T_target)\n    # Handle extrapolation for maturities shorter than the first calibrated point by scaling (approximately linear in T)\n    if T_target <= Ts[0]:\n        # Compute scaling factor relative to the first maturity (guarding against division by zero)\n        lam = T_target / Ts[0] if Ts[0] > 0 else 0.0\n        # Scale theta down proportionally for short maturities\n        theta_T = lam * theta[0]\n        # Scale psi down proportionally for short maturities\n        psi_T = lam * psi[0]\n        # Keep rho fixed at the first calibrated rho\n        rho_T = rho[0]\n        # Return extrapolated parameters\n        return theta_T, psi_T, rho_T\n    # Handle extrapolation for maturities longer than the last calibrated point\n    if T_target >= Ts[-1]:\n        # Estimate a nonnegative slope for theta using the last two points if available\n        if len(Ts) >= 2 and Ts[-1] > Ts[-2]:\n            slope = max((theta[-1] - theta[-2]) / (Ts[-1] - Ts[-2]), 0.0)\n        else:\n            slope = 0.0\n        # Extrapolate theta forward using the slope (clipped to be nondecreasing)\n        theta_T = theta[-1] + slope * (T_target - Ts[-1])\n        # Hold psi constant beyond the last point\n        psi_T = psi[-1]\n        # Hold rho constant beyond the last point\n        rho_T = rho[-1]\n        # Return extrapolated parameters\n        return theta_T, psi_T, rho_T\n        \n    # Find the index i such that Ts[i] <= T_target < Ts[i+1]\n    i = np.searchsorted(Ts, T_target) - 1\n    # Clamp i to a valid interval index range\n    i = max(0, min(i, len(Ts) - 2))\n    # Extract the bracketing maturities\n    T0, T1 = Ts[i], Ts[i + 1]\n    # Compute linear interpolation weight lambda in [0,1]\n    lam = (T_target - T0) / (T1 - T0)\n    # Linearly interpolate theta between the two surrounding maturities\n    theta_T = (1.0 - lam) * theta[i] + lam * theta[i + 1]\n    # Linearly interpolate psi between the two surrounding maturities\n    psi_T = (1.0 - lam) * psi[i] + lam * psi[i + 1]\n    # Interpolate the product rho*psi (more stable than interpolating rho directly)\n    rho_psi_T = (1.0 - lam) * rho[i] * psi[i] + lam * rho[i + 1] * psi[i + 1]\n    # Recover rho at T_target by dividing by interpolated psi\n    rho_T = rho_psi_T / psi_T\n    # Return interpolated parameters\n    return theta_T, psi_T, rho_T\n\n# Calibrate eSSVI parameters for a single maturity slice given observed (k, w) points and optional previous-slice constraints\ndef calibrate_essvi_slice(k_obs, w_obs,\n                          prev_theta=None, prev_psi=None, prev_rho=None,\n                          rho_grid=None):\n    # Convert observed k values to a float NumPy array\n    k_obs = np.asarray(k_obs, dtype=float)\n    # Convert observed total variances to a float NumPy array\n    w_obs = np.asarray(w_obs, dtype=float)\n    # Find the index of the observation closest to ATM (k ≈ 0)\n    idx_star = int(np.argmin(np.abs(k_obs)))\n    # Extract that near-ATM log-forward moneyness value\n    k_star = float(k_obs[idx_star])\n    # Use the near-ATM observed total variance as the anchor theta_star\n    theta_star = float(w_obs[idx_star])\n    # If no rho grid is provided, create a default grid spanning (-0.99, 0.99)\n    if rho_grid is None:\n        rho_grid = np.linspace(-0.99, 0.99, 41)\n    # Define a large penalty value for constraint violations or invalid model values\n    big = 1e9\n    # Initialize the best objective value as +infinity\n    best_err = np.inf\n    # Initialize the best rho candidate\n    best_rho = 0.0\n    # Initialize the best psi candidate\n    best_psi = 1e-4\n    # Initialize the best theta candidate (start from theta_star)\n    best_theta = theta_star\n    # Loop over candidate rho values and optimize psi for each rho\n    for rho in rho_grid:\n        # Compute denom = 1 + |rho| used in upper bound formulas\n        denom = 1.0 + abs(rho)\n        # Compute the discriminant-like term used to derive a feasible psi upper bound\n        tmp = 4.0 * rho * rho * k_star * k_star / (denom * denom) + 4.0 * theta_star / denom\n        # Skip rho values that yield nonpositive tmp (no real sqrt => no feasible psi_plus)\n        if tmp <= 0.0:\n            continue\n        # Compute psi_plus from the derived feasibility condition\n        psi_plus = -2.0 * rho * k_star / denom + np.sqrt(tmp)\n        # Set an upper bound on psi using both psi_plus and the calendar constraint 4/(1+|rho|)\n        psi_upper = min(psi_plus, 4.0 / denom)\n        # Set a small positive lower bound on psi to avoid degeneracy\n        psi_lower = 1e-8\n        # If previous slice parameters are available, enforce monotonic/calendar-style lower bounds on psi\n        if prev_theta is not None and prev_psi is not None and prev_rho is not None:\n            # Enforce that psi does not decrease (with a small epsilon) across maturities\n            psi_lower = max(psi_lower, prev_psi + 1e-8)\n            # Precompute denominators for additional calendar constraints\n            denom1 = 1.0 - rho\n            denom2 = 1.0 + rho\n            # Skip invalid rho values that would make denominators nonpositive\n            if denom1 <= 0 or denom2 <= 0:\n                continue\n            # Compute constraint-based minimum psi implied by (1-rho) scaling from previous slice\n            psi_min_cal1 = prev_psi * (1.0 - prev_rho) / denom1\n            # Compute constraint-based minimum psi implied by (1+rho) scaling from previous slice\n            psi_min_cal2 = prev_psi * (1.0 + prev_rho) / denom2\n            # Update psi_lower to satisfy both calendar constraints\n            psi_lower = max(psi_lower, psi_min_cal1, psi_min_cal2)\n        # Skip this rho if feasible interval is empty or inverted\n        if psi_lower >= psi_upper:\n            continue\n        # Define the objective function in psi for a fixed rho\n        \n        def obj(psi):\n            # Compute theta from the ATM anchoring relationship theta = theta_star - rho*psi*k_star\n            theta = theta_star - rho * psi * k_star\n            # Penalize if theta is too small or negative\n            if theta <= 1e-8:\n                return big + 1e4 * (1e-8 - theta) ** 2\n            # Penalize if theta would decrease relative to previous slice (calendar monotonicity)\n            if prev_theta is not None and theta < prev_theta:\n                return big + 1e4 * (prev_theta - theta) ** 2\n            # Compute model-implied total variance at observed k points\n            w_model = essvi_total_variance(k_obs, theta, psi, rho)\n            # Penalize any non-finite model output (NaN/inf)\n            if not np.all(np.isfinite(w_model)):\n                return big\n            # Return mean squared error between model and observed total variances\n            return np.mean((w_model - w_obs) ** 2)\n        # Run bounded 1D optimization to find psi minimizing the objective for this rho\n        res = minimize_scalar(\n            obj,\n            bounds=(psi_lower, psi_upper),\n            method=\"bounded\",\n            options={\"xatol\": 1e-8, \"maxiter\": 200},\n        )\n        # Skip if optimizer did not converge successfully\n        if not res.success:\n            continue\n        # Extract optimized psi from the optimizer result\n        psi_opt = float(res.x)\n        # Recompute theta corresponding to the optimized psi\n        theta_opt = theta_star - rho * psi_opt * k_star\n        # Skip if theta is not strictly positive\n        if theta_opt <= 0:\n            continue\n        # Extract the optimized objective value (MSE)\n        err = float(res.fun)\n        # Update the best parameters if this rho/psi combination improves the fit\n        if err < best_err:\n            best_err = err\n            best_rho = rho\n            best_psi = psi_opt\n            best_theta = theta_opt\n    # Return calibrated (theta, psi, rho) plus anchor diagnostics and the best MSE\n    return best_theta, best_psi, best_rho, k_star, theta_star, best_err\n\n# Calibrate eSSVI parameters across all maturity slices present in the DataFrame\ndef calibrate_essvi_slices(df):\n    # Extract unique maturities (TTM) and sort them ascending for sequential calendar-consistent calibration\n    Ts = np.sort(df[\"TTM\"].unique())\n    # Initialize a list to collect calibrated parameter rows\n    params_rows = []\n    # Initialize previous-slice parameters (used to enforce monotonic/calendar constraints)\n    prev_theta = prev_psi = prev_rho = None\n    # Iterate through each maturity slice in ascending order\n    for i, T in enumerate(Ts):\n        # Extract the slice of quotes corresponding to the given maturity\n        sli = df.loc[df[\"TTM\"] == T].copy()\n        # Pull observed log-forward moneyness values as a NumPy array\n        k_obs = sli[\"LogMK\"].to_numpy()\n        # Pull observed total variances as a NumPy array\n        w_obs = sli[\"w\"].to_numpy()\n        # Calibrate eSSVI for this maturity slice using constraints from the previous slice if available\n        theta, psi, rho, k_star, theta_star, mse = calibrate_essvi_slice(\n            k_obs, w_obs,\n            prev_theta=prev_theta,\n            prev_psi=prev_psi,\n            prev_rho=prev_rho,\n        )\n        # Convert mean squared error into root mean squared error for interpretability\n        rmse = float(np.sqrt(mse))\n        # Append the calibrated parameters and diagnostics for this maturity to the results list\n        params_rows.append({\n            \"T\": T,\n            \"theta\": theta,\n            \"psi\": psi,\n            \"rho\": rho,\n            \"k_star\": k_star,\n            \"theta_star\": theta_star,\n            \"rmse\": rmse,\n            \"n_quotes\": len(sli),\n        })\n        # Update previous-slice parameters for the next maturity calibration\n        prev_theta, prev_psi, prev_rho = theta, psi, rho\n    # Create a DataFrame of calibrated parameters and sort by maturity\n    essvi_params = (\n        pd.DataFrame(params_rows)\n        .sort_values(\"T\")\n        .reset_index(drop=True)\n    )\n    # Return the calibrated term-structure parameter DataFrame\n    return essvi_params\n    \n# Build an implied-vol surface on a (moneyness, maturity) grid from calibrated eSSVI parameters and a given rate\ndef build_essvi_surface_on_grid(essvi_params, moneyness_grid, ttm_grid, rate):\n    # Extract calibrated maturities as a NumPy array\n    Ts = essvi_params[\"T\"].to_numpy()\n    # Extract calibrated theta values as a NumPy array\n    theta_arr = essvi_params[\"theta\"].to_numpy()\n    # Extract calibrated psi values as a NumPy array\n    psi_arr = essvi_params[\"psi\"].to_numpy()\n    # Extract calibrated rho values as a NumPy array\n    rho_arr = essvi_params[\"rho\"].to_numpy()\n    # Convert moneyness grid to float NumPy array for consistent numeric operations\n    moneyness_grid = np.asarray(moneyness_grid, dtype=float)\n    # Compute log spot-moneyness log(K/S) once, reused across maturities\n    log_m = np.log(moneyness_grid)\n    # Initialize a list of row dictionaries to build the grid DataFrame\n    rows = []\n    # Loop over target maturities in the surface grid\n    for T in ttm_grid:\n        # Interpolate (theta, psi, rho) to this maturity T using the calibrated term structure\n        theta_T, psi_T, rho_T = interpolate_essvi_params(\n            T, Ts, theta_arr, psi_arr, rho_arr\n        )\n        # Compute log-forward moneyness k = log(K/S) - log(F/S) = log(m) - r*T\n        k_grid = log_m - rate * T\n        # If interpolated parameters are invalid, fall back to zero variance on this row\n        if theta_T <= 0 or psi_T <= 0:\n            w_row = np.zeros_like(k_grid, dtype=float)\n        else:\n            # Compute total variance across the k grid using eSSVI\n            w_row = essvi_total_variance(k_grid, theta_T, psi_T, rho_T)\n        # Store each grid point as a row (T, moneyness, k, w) for later pivoting/analysis\n        for mg, k_val, wv in zip(moneyness_grid, k_grid, w_row):\n            rows.append({\n                \"T\": float(T),\n                \"moneyness\": float(mg),\n                \"k\": float(k_val),\n                \"w\": max(float(wv), 0.0),\n            })\n    # Create a DataFrame from collected rows and sort for stable ordering\n    grid_essvi = (\n        pd.DataFrame(rows)\n        .sort_values([\"T\", \"moneyness\"])\n        .reset_index(drop=True)\n    )\n    # Convert total variance w to implied volatility iv = sqrt(w/T), safely handling T=0\n    grid_essvi[\"iv\"] = np.sqrt(\n        np.where(grid_essvi[\"T\"] > 0.0, grid_essvi[\"w\"] / grid_essvi[\"T\"], 0.0)\n    )\n    # Return the gridded eSSVI surface DataFrame\n    return grid_essvi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.354871Z","iopub.execute_input":"2025-12-28T22:26:26.355184Z","iopub.status.idle":"2025-12-28T22:26:26.390218Z","shell.execute_reply.started":"2025-12-28T22:26:26.355153Z","shell.execute_reply":"2025-12-28T22:26:26.389083Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define a function that builds a 4D \"vol cube\" tensor (samples × channels × TTM × moneyness) from daily option data\ndef formating_vol_cube(df, dates):\n    # Initialize an empty cube with a dummy first sample to simplify concatenation inside the loop\n    vol_cube = np.empty((1, 5, 20, 20))\n    # Loop over each quote date to build one surface (one sample) per date\n    for date in dates:\n        # Look up the risk-free rate applicable to this quote date (from the external cleaned rate DataFrame)\n        rate = get_rate(df_risk_free_rates, date)\n        # Look up the underlying spot price for this quote date from the option-chain DataFrame\n        price = get_price(df, date)\n        # Filter the full option-chain DataFrame down to only rows for the current quote date\n        df_data = df[df[\"QUOTE_DATE\"] == date]\n        # Calibrate eSSVI parameters across all maturities available on this quote date\n        essvi_params = calibrate_essvi_slices(df_data)\n        # Build the eSSVI implied-vol surface on the predefined (ttm_grid × moneyness_grid) grid\n        grid_essvi = build_essvi_surface_on_grid(\n            essvi_params,\n            moneyness_grid=moneyness_grid,\n            ttm_grid=ttm_grid,\n            rate=rate,)\n        # Pivot the long-form grid DataFrame into a 2D matrix of implied vols indexed by T and moneyness\n        df_grid_pivot = grid_essvi.pivot(index=\"T\", columns=\"moneyness\", values=\"iv\").sort_index().sort_index(axis=1)\n        # Convert the pivoted implied-vol surface into a NumPy array (shape: n_T × n_moneyness)\n        IV_grid = df_grid_pivot.to_numpy()\n        # Add a leading dimension so the surface can be stacked with other 2D \"channels\"\n        IV_grid_expanded = np.expand_dims(IV_grid, axis=0)\n        # Create a grid-shaped array filled with the scalar rate so it can be used as a channel\n        rate_expanded = np.full(IV_grid_expanded.shape, rate)\n        # Create a grid-shaped array filled with the scalar spot price so it can be used as a channel\n        price_expanded = np.full(IV_grid_expanded.shape, price)\n        # Stack the TTM grid channel with the implied-vol channel (channel dimension grows by concatenation on axis=0)\n        IV_grid_expanded = np.concatenate((TTM_grid_expanded, IV_grid_expanded), axis=0)\n        # Append the spot price channel to the channel stack\n        IV_grid_expanded = np.concatenate((IV_grid_expanded, price_expanded), axis=0)\n        # Append the rate channel to the channel stack\n        IV_grid_expanded = np.concatenate((IV_grid_expanded, rate_expanded), axis=0)\n        # Append the strike grid channel K = S * (K/S) by multiplying spot price by the moneyness grid\n        IV_grid_expanded = np.concatenate((IV_grid_expanded, price * M_grid_expanded), axis=0)\n        # Add a leading sample dimension so this date becomes one sample in the dataset\n        IV_grid_expanded = np.expand_dims(IV_grid_expanded, axis=0)\n        # Concatenate this sample onto the growing vol cube along the sample axis\n        vol_cube = np.concatenate((vol_cube, IV_grid_expanded), axis=0)\n    # Remove the initial dummy sample row that was only used to make concatenation convenient\n    vol_cube = np.delete(vol_cube, 0, axis=0)\n    # Return the final vol cube tensor (shape: n_dates × 5 × 20 × 20)\n    return vol_cube","metadata":{"execution":{"iopub.status.busy":"2025-12-28T22:26:26.393637Z","iopub.execute_input":"2025-12-28T22:26:26.393918Z","iopub.status.idle":"2025-12-28T22:26:26.422054Z","shell.execute_reply.started":"2025-12-28T22:26:26.393897Z","shell.execute_reply":"2025-12-28T22:26:26.420608Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create a 1D grid of spot-moneyness values K/S from 0.90 up to (but not including) 1.09 in steps of 0.01\nmoneyness_grid = np.arange(0.9, 1.09, 0.01)\n# Create a 1D grid of maturities from 30 to 600 days (inclusive) in steps of 30, then convert days to years by dividing by 365\nttm_grid = np.arange(30, 600 + 1, 30) / 365\n# Build 2D mesh grids so every maturity is paired with every moneyness (used to construct grid-shaped channels)\nM_grid, TTM_grid = np.meshgrid(moneyness_grid, ttm_grid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.423220Z","iopub.execute_input":"2025-12-28T22:26:26.423560Z","iopub.status.idle":"2025-12-28T22:26:26.451678Z","shell.execute_reply.started":"2025-12-28T22:26:26.423502Z","shell.execute_reply":"2025-12-28T22:26:26.450411Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Add a leading channel dimension to the moneyness grid so it can be stacked with other surface channels\nM_grid_expanded = np.expand_dims(M_grid, axis=0)\n# Add a leading channel dimension to the time-to-maturity grid so it can be stacked with other surface channels\nTTM_grid_expanded = np.expand_dims(TTM_grid, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.452601Z","iopub.execute_input":"2025-12-28T22:26:26.452866Z","iopub.status.idle":"2025-12-28T22:26:26.473492Z","shell.execute_reply.started":"2025-12-28T22:26:26.452839Z","shell.execute_reply":"2025-12-28T22:26:26.472543Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Clean and interpolate the raw risk-free rate dataset to produce a daily, fully populated rate time series\ndf_risk_free_rates = rates_clean(risk_free_rates)\n# Save the cleaned and interpolated daily risk-free rate DataFrame to a CSV file for inspection or reuse\ndf_risk_free_rates.to_csv(\"rates_interpolated.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.474688Z","iopub.execute_input":"2025-12-28T22:26:26.475065Z","iopub.status.idle":"2025-12-28T22:26:26.556351Z","shell.execute_reply.started":"2025-12-28T22:26:26.475042Z","shell.execute_reply":"2025-12-28T22:26:26.555048Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/3456462164.py:4: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Clean and standardize the raw training option-chain data (select columns, parse dates, compute TTM/moneyness, coerce numerics)\ndf_option_chains_train = clean_data(option_chains_train)\n# Merge the cleaned option-chain data with the daily risk-free rate series by matching QUOTE_DATE to the rate \"Date\"\ndf_option_chains_rates_train = df_option_chains_train.merge(\n    df_risk_free_rates,\n    left_on=\"QUOTE_DATE\",\n    right_on=\"Date\",\n    how=\"left\",\n)\n# Add derived forward/total-variance features (F, LogMK, w) and drop the redundant merge key column\ndf_option_chains_rates_clean_train = df_option_chains_rates_cleaning(df_option_chains_rates_train)\n# Extract the sorted list of unique quote dates present in the enriched training dataset\ndates_train = dates_extraction(df_option_chains_rates_clean_train)\n# Build the training vol cube tensor by calibrating eSSVI and gridding implied vols for each quote date\nvol_cube_train = formating_vol_cube(df_option_chains_rates_clean_train, dates_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:26:26.557481Z","iopub.execute_input":"2025-12-28T22:26:26.557842Z","iopub.status.idle":"2025-12-28T22:34:44.149406Z","shell.execute_reply.started":"2025-12-28T22:26:26.557813Z","shell.execute_reply":"2025-12-28T22:34:44.148130Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Save the cleaned training option-chain DataFrame to CSV for verification or reuse\ndf_option_chains_train.to_csv(\"clean_train_chains.csv\")\n# Save the list of training quote dates to CSV for reference or downstream processing\ndates_train.to_csv(\"dates_train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:34:44.150642Z","iopub.execute_input":"2025-12-28T22:34:44.151009Z","iopub.status.idle":"2025-12-28T22:34:55.786036Z","shell.execute_reply.started":"2025-12-28T22:34:44.150978Z","shell.execute_reply":"2025-12-28T22:34:55.784498Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Clean and standardize the raw test option-chain data (select columns, parse dates, compute TTM/moneyness, coerce numerics)\ndf_option_chains_test = clean_data(option_chains_test)\n# Merge the cleaned test option-chain data with the daily risk-free rate series by matching QUOTE_DATE to the rate \"Date\"\ndf_option_chains_rates_test = df_option_chains_test.merge(\n    df_risk_free_rates,\n    left_on=\"QUOTE_DATE\",\n    right_on=\"Date\",\n    how=\"left\",\n)\n# Add derived forward/total-variance features (F, LogMK, w) and drop the redundant merge key column\ndf_option_chains_rates_clean_test = df_option_chains_rates_cleaning(df_option_chains_rates_test)\n# Extract the sorted list of unique quote dates present in the enriched test dataset\ndates_test = dates_extraction(df_option_chains_rates_clean_test)\n# Build the test vol cube tensor by calibrating eSSVI and gridding implied vols for each quote date\nvol_cube_test = formating_vol_cube(df_option_chains_rates_clean_test, dates_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:34:55.787796Z","iopub.execute_input":"2025-12-28T22:34:55.788169Z","iopub.status.idle":"2025-12-28T22:40:15.886444Z","shell.execute_reply.started":"2025-12-28T22:34:55.788137Z","shell.execute_reply":"2025-12-28T22:40:15.885105Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Save the cleaned test option-chain DataFrame to CSV for verification or reuse\ndf_option_chains_test.to_csv(\"clean_test_chains.csv\")\n# Save the list of test quote dates to CSV for reference or downstream processing\ndates_test.to_csv(\"dates_test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:15.887560Z","iopub.execute_input":"2025-12-28T22:40:15.888080Z","iopub.status.idle":"2025-12-28T22:40:22.067449Z","shell.execute_reply.started":"2025-12-28T22:40:15.888050Z","shell.execute_reply":"2025-12-28T22:40:22.065681Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Save the training and test vol cube tensors into a single compressed NumPy archive file\nnp.savez_compressed(\"vol_cube.npz\", train=vol_cube_train, test=vol_cube_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:22.068694Z","iopub.execute_input":"2025-12-28T22:40:22.068995Z","iopub.status.idle":"2025-12-28T22:40:22.702684Z","shell.execute_reply.started":"2025-12-28T22:40:22.068968Z","shell.execute_reply":"2025-12-28T22:40:22.701091Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Define a function that approximates the standard normal CDF Φ(x) for vector inputs\ndef norm_cdf(x: np.ndarray) -> np.ndarray:\n    # Convert input to a NumPy float array to ensure vectorized numerical operations\n    x = np.asarray(x, dtype=float)\n    # Extract the sign of x so the approximation can be applied to |x| and then re-signed\n    sign = np.sign(x)\n    # Compute |x|/sqrt(2) because erf-based CDF formulas use x / sqrt(2)\n    x_abs = np.abs(x) / np.sqrt(2.0)\n    # Set approximation coefficients for a polynomial erf approximation (Abramowitz–Stegun style)\n    a1 = 0.254829592\n    # Set the second coefficient for the erf approximation polynomial\n    a2 = -0.284496736\n    # Set the third coefficient for the erf approximation polynomial\n    a3 = 1.421413741\n    # Set the fourth coefficient for the erf approximation polynomial\n    a4 = -1.453152027\n    # Set the fifth coefficient for the erf approximation polynomial\n    a5 = 1.061405429\n    # Set the constant p used in the rational approximation term t = 1/(1 + p*x)\n    p  = 0.3275911\n    # Compute t = 1/(1 + p*x_abs) which drives the polynomial evaluation\n    t = 1.0 / (1.0 + p * x_abs)\n    # Compute an approximation of erf(x_abs) using a nested polynomial (Horner form) times exp(-x_abs^2)\n    erf_approx = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * np.exp(-x_abs * x_abs)\n    # Re-apply the sign to extend the approximation from x>=0 to all real x\n    erf_approx *= sign\n    # Convert erf approximation to normal CDF using Φ(x) = 0.5 * (1 + erf(x/sqrt(2)))\n    return 0.5 * (1.0 + erf_approx)\n\n# Define a vectorized Black–Scholes call pricer given spot S, strike K, maturity T, rate r, and volatility sigma\ndef black_scholes_call(S, K, T, r, sigma):\n    # Convert spot to a NumPy float array for broadcasting over grids\n    S = np.asarray(S, dtype=float)\n    # Convert strike to a NumPy float array for broadcasting over grids\n    K = np.asarray(K, dtype=float)\n    # Convert maturity to a NumPy float array for broadcasting over grids\n    T = np.asarray(T, dtype=float)\n    # Convert risk-free rate to a NumPy float array for broadcasting over grids\n    r = np.asarray(r, dtype=float)\n    # Convert volatility to a NumPy float array for broadcasting over grids\n    sigma = np.asarray(sigma, dtype=float)\n    # Define a small epsilon to prevent division by zero in sigma and T\n    eps = 1e-12\n    # Floor volatility at eps to avoid numerical blow-ups when sigma is near zero\n    sigma = np.maximum(sigma, eps)\n    # Floor maturity at eps to avoid division by zero and log issues when T is near zero\n    T = np.maximum(T, eps)\n    # Compute sqrt(T) once since it is used multiple times\n    sqrtT = np.sqrt(T)\n    # Compute the Black–Scholes d1 term\n    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * sqrtT)\n    # Compute the Black–Scholes d2 term\n    d2 = d1 - sigma * sqrtT\n    # Compute Φ(d1) using the normal CDF approximation\n    Nd1 = norm_cdf(d1)\n    # Compute Φ(d2) using the normal CDF approximation\n    Nd2 = norm_cdf(d2)\n    # Return the Black–Scholes call price C = S Φ(d1) - K e^{-rT} Φ(d2)\n    return S * Nd1 - K * np.exp(-r * T) * Nd2\n\n# Define a function that checks several static no-arbitrage conditions on a call surface implied by (T,K,vol)\ndef check_call_surface_no_arbitrage(\n    S0: float,\n    r: float,\n    T_grid: np.ndarray,\n    K_grid: np.ndarray,\n    vol_grid: np.ndarray,\n    tol: float = 1e-8,) -> dict:\n    # Convert the maturity grid to a NumPy float array\n    T_grid = np.asarray(T_grid, dtype=float)\n    # Convert the strike grid to a NumPy float array\n    K_grid = np.asarray(K_grid, dtype=float)\n    # Convert the volatility surface to a NumPy float array\n    vol_grid = np.asarray(vol_grid, dtype=float)\n    # Read the surface dimensions: number of maturities and number of strikes\n    n_T, n_K = vol_grid.shape\n    # Assert that the maturity grid length matches the surface first dimension\n    assert T_grid.shape == (n_T,)\n    # Assert that the strike grid length matches the surface second dimension\n    assert K_grid.shape == (n_K,)\n    # Build a maturity matrix by repeating T_grid across strike columns (shape: n_T × n_K)\n    T_mat = T_grid[:, None] * np.ones((1, n_K))\n    # Build a strike matrix by repeating K_grid across maturity rows (shape: n_T × n_K)\n    K_mat = np.ones((n_T, 1)) * K_grid[None, :]\n    # Compute the call price surface from the vol surface using Black–Scholes\n    C = black_scholes_call(S0, K_mat, T_mat, r, vol_grid)\n    # Compute the intrinsic lower bound max(0, S0 - K e^{-rT}) for each grid point\n    lower_bound = np.maximum(0.0, S0 - K_mat * np.exp(-r * T_mat))\n    # Set the simple upper bound for a call price, which is S0\n    upper_bound = S0\n    # Flag points where call prices violate the lower bound (with tolerance)\n    bound_low_viol  = C + tol < lower_bound\n    # Flag points where call prices violate the upper bound (with tolerance)\n    bound_high_viol = C - tol > upper_bound\n    # Compute first differences in strike direction (proxy for ∂C/∂K up to scaling)\n    dC_dK = np.diff(C, axis=1)\n    # Flag monotonicity violations: call prices must be nonincreasing in strike, so differences should be <= 0\n    monoK_viol = dC_dK > tol\n    # Compute strike spacing ΔK for finite-difference slope calculations\n    dK = np.diff(K_grid)\n    # Compute discrete slopes (C(K_{j+1}) - C(K_j)) / ΔK for each maturity row\n    slopes = (C[:, 1:] - C[:, :-1]) / dK[None, :]\n    # Compute differences of slopes (proxy for second derivative in K)\n    slope_diff = np.diff(slopes, axis=1)\n    # Flag convexity violations: call prices must be convex in strike, so slope differences should be >= 0\n    convex_viol = slope_diff < -tol\n    # Compute first differences in maturity direction (proxy for ∂C/∂T up to scaling)\n    dC_dT = np.diff(C, axis=0)\n    # Flag calendar violations: call prices should be nondecreasing in maturity (no calendar arbitrage)\n    calendar_viol = dC_dT < -tol\n    # Return a summary dictionary containing whether any violation occurred and counts of each type\n    return {\n        \"any_violation\": (\n            bound_low_viol.any()\n            or bound_high_viol.any()\n            or monoK_viol.any()\n            or convex_viol.any()\n            or calendar_viol.any()\n        ),\n        \"bound_low_count\": int(bound_low_viol.sum()),\n        \"bound_high_count\": int(bound_high_viol.sum()),\n        \"monoK_count\": int(monoK_viol.sum()),\n        \"convex_count\": int(convex_viol.sum()),\n        \"calendar_count\": int(calendar_viol.sum()),\n    }\n\n# Define a helper that extracts grids/parameters from one vol-cube sample and runs the no-arbitrage checker\ndef check_sample_from_vol_cube(sample, tol: float = 1e-8):\n    # Extract channel 0: maturity grid replicated across strikes (shape: n_T × n_K)\n    ch0 = sample[0]   # T grid replicated across strikes\n    # Extract channel 1: implied volatility surface (shape: n_T × n_K)\n    ch1 = sample[1]   # vol surface\n    # Extract channel 2: spot price grid (constant across the surface)\n    ch2 = sample[2]   # spot\n    # Extract channel 3: rate grid (constant across the surface)\n    ch3 = sample[3]   # r-like scalar (constant on grid)\n    # Extract channel 4: strike grid replicated across maturities (shape: n_T × n_K)\n    ch4 = sample[4]   # K grid replicated across maturities\n    # Recover the 1D maturity vector by taking the first strike column (all columns are identical in this channel)\n    T_grid = ch0[:, 0]\n    # Recover the 1D strike vector by taking the first maturity row (all rows are identical in this channel)\n    K_grid = ch4[0, :]\n    # Recover the scalar spot price from any grid entry and cast to Python float\n    S0 = float(ch2[0, 0])\n    # Recover the scalar rate from any grid entry and cast to Python float\n    r = float(ch3[0, 0])\n    # Run the no-arbitrage checks on the implied call surface generated from the vol surface\n    return check_call_surface_no_arbitrage(\n        S0=S0,\n        r=r,\n        T_grid=T_grid,\n        K_grid=K_grid,\n        vol_grid=ch1,\n        tol=tol,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:22.704127Z","iopub.execute_input":"2025-12-28T22:40:22.704509Z","iopub.status.idle":"2025-12-28T22:40:22.724507Z","shell.execute_reply.started":"2025-12-28T22:40:22.704480Z","shell.execute_reply":"2025-12-28T22:40:22.723349Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Define a function that scans an entire vol-cube dataset and aggregates no-arbitrage violation statistics\ndef summarize_dataset_no_arb(vol_cube, tol: float = 1e-8):\n    # Read the number of samples (dates) stored in the first dimension of the vol cube\n    n = vol_cube.shape[0]\n    # Initialize a summary dictionary to track dataset-level counts and totals\n    summary = {\n        \"total_samples\": n,\n        \"no_arbitrage_samples\": 0,\n        \"arbitrage_samples\": 0,\n        \"bound_low_total\": 0,\n        \"bound_high_total\": 0,\n        \"monoK_total\": 0,\n        \"convex_total\": 0,\n        \"calendar_total\": 0,\n    }\n    # Track the index of the first sample where any arbitrage is detected\n    first_arb_idx = None\n    # Store the detailed violation counts for the first arbitrage sample for debugging\n    first_arb_details = None\n    # Loop over each sample in the dataset\n    for i in range(n):\n        # Run the per-sample arbitrage checks by extracting grids from vol_cube[i]\n        res = check_sample_from_vol_cube(vol_cube[i], tol=tol)\n        # If any arbitrage condition is violated, update arbitrage counters and totals\n        if res[\"any_violation\"]:\n            summary[\"arbitrage_samples\"] += 1\n            summary[\"bound_low_total\"]  += res[\"bound_low_count\"]\n            summary[\"bound_high_total\"] += res[\"bound_high_count\"]\n            summary[\"monoK_total\"]      += res[\"monoK_count\"]\n            summary[\"convex_total\"]     += res[\"convex_count\"]\n            summary[\"calendar_total\"]   += res[\"calendar_count\"]\n            # If this is the first arbitrage sample encountered, record its index and details\n            if first_arb_idx is None:\n                first_arb_idx = i\n                first_arb_details = res\n        # Otherwise, count this sample as no-arbitrage\n        else:\n            summary[\"no_arbitrage_samples\"] += 1\n    # Add the index of the first arbitrage sample (or None if no arbitrage was found)\n    summary[\"first_arbitrage_index\"] = first_arb_idx\n    # Add the detailed results for the first arbitrage sample (or None if no arbitrage was found)\n    summary[\"first_arbitrage_details\"] = first_arb_details\n    # Return the completed dataset-level summary statistics\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:22.725665Z","iopub.execute_input":"2025-12-28T22:40:22.726053Z","iopub.status.idle":"2025-12-28T22:40:22.755404Z","shell.execute_reply.started":"2025-12-28T22:40:22.726009Z","shell.execute_reply":"2025-12-28T22:40:22.754034Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Run the no-arbitrage summary checks across all samples in the training vol cube using the default tolerance\nsummarize_dataset_no_arb(vol_cube_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:22.756829Z","iopub.execute_input":"2025-12-28T22:40:22.757207Z","iopub.status.idle":"2025-12-28T22:40:23.106538Z","shell.execute_reply.started":"2025-12-28T22:40:22.757177Z","shell.execute_reply":"2025-12-28T22:40:23.105383Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'total_samples': 1253,\n 'no_arbitrage_samples': 1176,\n 'arbitrage_samples': 77,\n 'bound_low_total': 0,\n 'bound_high_total': 0,\n 'monoK_total': 0,\n 'convex_total': 0,\n 'calendar_total': 1811,\n 'first_arbitrage_index': 137,\n 'first_arbitrage_details': {'any_violation': True,\n  'bound_low_count': 0,\n  'bound_high_count': 0,\n  'monoK_count': 0,\n  'convex_count': 0,\n  'calendar_count': 45}}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Run the no-arbitrage summary checks across all samples in the test vol cube using the default tolerance\nsummarize_dataset_no_arb(vol_cube_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T22:40:23.107561Z","iopub.execute_input":"2025-12-28T22:40:23.107822Z","iopub.status.idle":"2025-12-28T22:40:23.262496Z","shell.execute_reply.started":"2025-12-28T22:40:23.107802Z","shell.execute_reply":"2025-12-28T22:40:23.261569Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'total_samples': 570,\n 'no_arbitrage_samples': 565,\n 'arbitrage_samples': 5,\n 'bound_low_total': 0,\n 'bound_high_total': 0,\n 'monoK_total': 0,\n 'convex_total': 0,\n 'calendar_total': 45,\n 'first_arbitrage_index': 128,\n 'first_arbitrage_details': {'any_violation': True,\n  'bound_low_count': 0,\n  'bound_high_count': 0,\n  'monoK_count': 0,\n  'convex_count': 0,\n  'calendar_count': 14}}"},"metadata":{}}],"execution_count":22}]}
